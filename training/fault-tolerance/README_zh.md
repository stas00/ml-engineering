# README - 中文翻译

容错

无论你是拥有ML训练硬件还是按小时租用，由于ML领域的快速发展，在规定时间内完成训练非常重要。因此，如果你在睡觉时其中一个GPU故障或检查点存储空间不足导致训练中断，醒来后你会发现许多训练时间已经白白浪费了。

由于ML硬件成本高昂，很难像Web服务那样提供冗余故障转移解决方案。然而，通过一些简单的技巧，可以实现训练的容错。

由于大多数重要的训练任务都在SLURM环境中进行，所以会频繁提到这一点，但本章中的大部分见解也适用于其他任何训练环境。

## 总是计划多于所需节点的数量

GPU设备的实际使用情况表明它们容易发生故障。有时它们只是过热而关机，但可以恢复；而在其他情况下，它们可能直接损坏，需要更换。

随着你在几周/几个月内使用相同的节点，这种情况往往会逐渐改善，因为不良的节点会逐渐被替换。但是，如果你有幸收到一批新的GPU，尤其是新推出的早期GPU，预计会有相当比例的GPU会故障。

因此，如果你需要64个节点来完成训练，请确保有足够的备用节点，并研究如何快速替换故障节点，以防备用节点不够。

很难准确预测应该有多大的冗余百分比，但5-10%应该是合理的。如果你的时间紧迫，希望按时完成训练，安全边际应该更高。

一旦有备用节点可用，验证你的SLURM环境是否会自动移除任何有问题的节点，以便它可以自动用好的节点替换坏的节点。

如果你使用非SLURM调度器，请验证它是否也能无人值守地替换故障节点。

你还需要至少一个额外的节点来运行各种预防性看门狗（本章稍后讨论），可能还要用来离线保存检查点和执行清理工作。

## 排队多个训练任务

接下来的关键步骤是确保如果训练失败，有一个新的任务能够接替前一个任务。

因此，当开始训练时，不要使用：
```bash
sbatch train.slurm
```

而是要替换为：
```bash
sbatch --array=1-10%1 train.slurm
```

这告诉SLURM预订一个由10个作业组成的作业数组，如果其中一个作业正常完成或崩溃，它将立即安排下一个作业。

注释：`--array=1-10%1` 中的 `%1` 告诉SLURM以串行方式启动作业数组——一次一个作业。

如果你已经在没有这种设置的情况下开始了训练，可以通过使用 `--dependency` 参数轻松修复而不中止当前作业：
```bash
sbatch --array=1-10%1 --dependency=CURRENTLY_RUNNING_JOB_ID train.slurm
```

假设你启动的作业如下所示：

```bash
$ squeue -u `whoami` -o "%.10i %9P %20j %.8T %.10M %.8l %.6D %.20S %R"
     JOBID PARTITION NAME             STATE       TIME   TIME_LIM    NODES  START_TIME     NODELIST(REASON)
       87    prod    my-training-10b  RUNNING 2-15:52:19 1-16:00:00   64    2023-10-07T01:26:28 node-[1-63]
```

你会注意到当前的 `JOBID=87`，现在可以使用它：
```bash
sbatch --array=1-10%1 --dependency=87 train.slurm
```

然后新的状态将显示为：
```bash
$ squeue -u `whoami` -o "%.10i %9P %20j %.8T %.10M %.8l %.6D %.20S %R"
     JOBID PARTITION NAME             STATE       TIME   TIME_LIM    NODES  START_TIME     NODELIST(REASON)
       87    prod    my-training-10b  RUNNING 2-15:52:19 1-16:00:00   64    2023-10-07T01:26:28 node-[1-63]
 88_[10%1]   prod    my-training-10b  PENDING       0:00 1-16:00:00   64                    N/A (Dependency)
```

可以看到一个包含10个作业的数组（`88_[10%1]`）将在当前作业（`87`）完成或失败后立即启动。

当然，如果导致崩溃的条件仍然存在，后续的作业也会失败。例如，如果存储设备已满，无论重启多少次，训练都无法继续。我们稍后将讨论如何避免这种情况。

但是，由于训练崩溃的主要原因是GPU故障，确保自动移除故障节点并让新作业从一组新的节点开始，可以使训练从崩溃中平稳恢复。

在SLURM术语中，被移除的节点被赋予一个新的状态，称为“drain”。以下是一个假设的SLURM集群示例：

```bash
$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
prod*       up   infinite       4  drain node-[0-3]
prod*       up   infinite      47  alloc node-[4-51]
prod*       up   infinite      23   idle node-[52-73]
```

这里我们有47个节点正在使用（`alloc`），23个节点可用（`idle`），4个节点不可用（`drain`）。

系统管理员应定期检查被隔离的节点，修复或更换它们，并通过将其状态更改为`idle`使其再次可用于使用。

另一种方法是通过`--dependency`串联任务，如[这里](../../orchestration/slurm/users.md#request-allocation-via-dependency)所述。这两种方法也可以结合使用。

你怎么知道何时不应继续运行作业数组或串联任务呢？通常情况下，如果训练循环立即退出，表示任务已完成。但是，你还可以添加类似[关闭开关](#关闭开关)的功能，即使更容易使用，也可以防止作业数组运行。

## 更喜欢固定加速器分配而不是动态分配

通常，当你获得新的加速器节点，特别是最近推出的新类型加速器时，许多加速器可能会故障，使LLM训练变得非常棘手。对于新推出的加速器，早期的故障率可能高达10%，即使在后期阶段，故障率仍然很高。记住，如果你有8个加速器，即使一个加速器故障，从训练程序的角度来看，就相当于所有8个都故障了。

如果你使用固定的节点分配，在几个月后，不良的加速器会被淘汰，剩下的加速器故障的情况应该很少。尽管仍会发生，但这种情况会非常罕见。

确保你的供应商在加速器故障时给你新的加速器，而不是在冷却后将它们还给你（字面意义上）。例如，查看如何跟踪[NVIDIA GPU的UUID](../../compute/accelerator/nvidia/debug.md#如何检测是否再次获取到同一个故障节点)。这些瞬时故障在高负载下可能会重复出现，因此你需要真正的替换它们。

如果你使用动态分配，即使在一种新型加速器发布一年后，你依然会遇到大量故障的加速器，因为你可能会从其他用户那里拒绝分配节点。当然，有些云平台比其他平台更好地勤于更换不良硬件，问题是有很多加速器不会完全故障，当有人报告一个故障节点时，技术人员在尝试使用时可能看不到任何问题。如果用户只是释放了节点但没有报告它是坏的，而云提供商又不重新检查节点是否合格就给下一个用户使用，那么得到坏节点的概率极高。

## 频繁保存检查点

每当训练任务失败时，可能会丢失数小时的训练。这个问题可以通过频繁保存检查点来缓解。当训练恢复时，它将从最后一次保存的检查点继续。如果失败发生在上一次检查点保存后的12小时，那么12小时的训练就会丢失，需要重新做。如果训练使用数百个GPU，这可能会非常昂贵。

理论上，每10分钟保存一次检查点，只会失去10分钟的训练时间，但这也会大大延迟达到终点线的时间，因为大型模型无法快速保存，如果保存时间开始成为训练的瓶颈，这种方法反而会适得其反。

根据你的检查点保存方法和IO存储分区的速度，保存一个大型模型可能需要几十秒到几分钟不等。因此，保存频率的最佳选择介于两者之间。

计算非常简单——测量保存检查点所需的时间，乘以你想保存的次数，看看检查点保存将对总训练时间增加多少延迟。

使用场景：在训练BLOOM-176B期间，我们有一个非常快的GPFS NVMe文件系统，保存一个2.3TB的检查点仅需40秒，并且我们大约每3小时保存一次检查点。因为我们训练了约3个月，这意味着我们总共保存了约720个检查点（90天 × 24小时 ÷ 3小时）——这额外花费了8小时用于保存检查点（720次 × 40秒 ÷ 3600秒）——或者只占总训练时间的0.37%（8小时 ÷ （90天 × 24小时）。假设IO速度慢5倍，这将占训练时间的2%，这将非常显著。

注释：如果没有足够的本地存储，你必须将检查点离线存储到云端，确保两个最频繁的检查点保留在本地以允许快速恢复。之所以需要两个而不是一个，是因为如果在保存最后一个检查点时发生崩溃，这个检查点可能会损坏或未完成保存。

虽然这种方法增加了训练的开销，但保存训练检查点是非常有用的。因为这些检查点允许你在出现分歧时回滚很多步，有助于分析各种事件，而且现在很多训练在训练过程中会从单损失衡量评估切换到基于数据集的多基准评估，应用于每个检查点。后者可以在附加节点上进行而不会减慢训练速度。

## 基于多副本的容错

还有一种处理加速器故障的方法，不需要保存检查点。这种方法仅在训练期间使用至少两个模型副本时才有效。

请先回顾各种[模型并行技术](../model-parallelism)，以便理解这种方法。

- 如果使用某种形式的三维模型并行，即你有张量并行（TP）和/或管道并行（PP）和/或数据并行（DP），副本的数量等于DP的度。
- 如果使用混合ZeRO-DP并行，副本的数量等于混合副本的度。

例如，假设你的训练设置使用了TP=4、PP=2、DP=2的三维并行性——因此你有两个副本，每个副本使用8个GPU（`node0`和`node1`）（TP=4，PP=2 => `4*2=8`）——实际上，每个副本使用了一个完整的8-GPU节点。

此外，你还有一个备用备份节点`node2`，拥有8个空闲的GPU随时待命。

现在，假设在训练期间`node0.gpu0`失效。由于你还有第二个完好无损的数据副本，你可以切换到备用的8-GPU节点，RDMA复制第二个副本上的GPU数据，然后继续从你停止的地方训练。这是一个非常简化的解释，因为根据故障发生的迭代循环阶段的不同，恢复的细节会有很多不同。换句话说，这里有一个复杂的算法需要实现。

当然，在大规模训练中，你很可能有一百个活动节点和一小部分备用节点。

这种方法优于文件系统检查点保存，因为只有一次迭代的损失，而文件系统检查点保存可能会丢失数百次迭代。

我不了解有任何开源实现这种高级容错方法，但我们知道一些大公司内部使用这种方法。

## 关闭开关

在许多SLURM环境中，用户没有`sudo`权限，当一个用户启动了一个训练任务后去睡觉，如果出现问题，其他用户不能轻易停止训练并重新开始。

这就是在BLOOM-176B训练期间的情况，我们实现了一个关闭开关来解决这个问题。机制非常简单。训练循环在开始新迭代之前会轮询特定文件是否存在，如果文件存在，程序将保存检查点并退出，允许除启动先前训练的用户外的其他人更改并重新启动。在`main`函数的开头还添加了一个轮询，以便如果用户在睡觉时排队了一个长时间的任务数组，他们可以快速“烧穿”每个任务，迅速退出。

这也在[这里](../../orchestration/slurm/users.md#克服缺乏SLURM作业组所有权)讨论过。

这一功能有助于减少浪费的训练时间。

## 保存开关

在提到关闭开关的同时，可能值得快速提及它的表亲——保存开关。类似于关闭开关，保存开关是一种变体，当训练循环发现保存开关文件出现时——它将保存一个检查点，但将继续训练。它还会自动从文件系统中删除保存开关，以免在每次迭代后意外开始保存检查点。

对于那些观察训练图表的人来说，这一功能非常有用。如果你在训练损失或其他训练指标中看到有趣或关键的情况，可以快速要求训练程序保存感兴趣的检查点，并能够在以后随时重现当前情况。

该功能的主要用途是在观察训练损失峰值和发散时。

（自注：这部分更适合放在不稳定章节）

## 预防

避免训练时间损失的最简单方法是防止某些类型的问题发生。虽然你不能阻止GPU故障，除了确保提供充足的冷却外，你肯定可以确保有足够的磁盘空间供接下来几天的训练使用。这通常通过运行预定的看门狗来实现，这些看门狗监控各种资源并在问题发生之前很久就发出警报。

### 预定看门狗

在讨论各种看门狗之前，你必须有一个机制来运行预定任务。在Unix世界中，这是通过[`crontab`设施](https://en.wikipedia.org/wiki/Cron)实现的。

以下是如何每小时启动`~/bin/watch-fs.sh`的一个例子：
```bash
0 * * * * ~/bin/watch-fs.sh
```
上述链接解释了如何配置crontab任务以在其他频率下运行。

要设置crontab，请执行`crontab -e`并检查已安排的任务`crontab -l`。

我在这里不深入细节的原因是，许多SLURM环境不提供访问`crontab`设施的权限。因此，你需要使用其他方法来调度任务。

[Crontab模拟](../../orchestration/slurm/users.md#crontab-emulation)一节讨论了如何实现类似crontab的SLURM模拟，以及[自维持的SLURM任务](../../orchestration/slurm/users.md#self-perpetuating-slurm-jobs)。

### 通知设施

然后你需要一个或多个通知设施。

最简单的一种是使用电子邮件发送警报。为了让这起作用，你需要确保可以从SLURM作业发送电子邮件。如果尚未提供此功能，你可以请求你的系统管理员提供，或者你也可以使用外部SMTP服务器提供商。

除了电子邮件，你还可以设置其他通知，比如短信警报和/或如果你使用Slack，向你选择的频道发送Slack通知。

一旦你了解了如何调度看门狗并且有了一个工作的通知设施，让我们接下来讨论关键的看门狗。

### 检查作业是否运行的看门狗

最明显的看门狗是检查是否有正在进行的训练SLURM作业或更多作业被安排运行。

以下是一个在BLOOM-176B训练期间使用的例子[slurm-status.py](slurm-status.py)。这个看门狗会在作业既没有运行也没有被安排时发送电子邮件，并且它还将检查结果输出到主训练的日志文件中。由于我们使用了[Crontab模拟](../../orchestration/slurm/users.md#crontab-emulation)，我们只需将[slurm-status.slurm](slurm-status.slurm)放入`cron/cron.hourly/`目录，以前启动的SLURM crontab模拟调度器将大约每小时运行一次这个检查。

SLURM作业的关键部分是：
```bash
tools/slurm-status.py --job-name $WATCH_SLURM_NAME 2>&1 | tee -a $MAIN_LOG_FILE
```
这告诉脚本要监视哪个作业名称，并且你还可以看到它记录到了日志文件中。

例如，如果你用以下命令启动脚本：
```bash
tools/slurm-status.py --job-name my-training-10b
```
并且当前状态报告显示：
```bash
$ squeue -u `whoami` -o "%.10i %9P %20j %.8T %.10M %.8l %.6D %.20S %R"
  JOBID    PARTITION NAME             STATE       TIME   TIME_LIM    NODES  START_TIME     NODELIST(REASON)
    87     prod      my-training-10b  RUNNING 2-15:52:19 1-16:00:00  64    2023-10-07T01:26:28 node-[1-63]
```
那么一切正常。但如果`my-training-10b`作业没有显示，警报将被发送。

你现在可以对这些脚本进行最小修改以适应你的需求，只需编辑路径和电子邮件地址即可。如果不是你启动了作业，则将`whoami`替换为启动作业的用户名。`whoami`仅在你启动作业时才有效。