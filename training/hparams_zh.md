# hparams - 中文翻译

# 选择训练超参数和模型初始化

找到一组好的超参数和模型初始化的最简单方法是从一个已知成功的类似训练中获取。这里有一个[公开训练LLM/VLM日志集合](../resources/README.md#publicly-available-training-llmvlm-logbooks)可以帮助你入手。另一个常见的来源是论文，如果它们披露了这些信息的话。如果他们没有公布，你也可以尝试联系作者并询问这些细节。

## 全局批量大小递增

如果你打算使用非常大的GBS（比如1024或2048样本甚至更高）进行训练，在刚开始时就将如此大的批量大小提供给模型是非常浪费的。此时它完全是随机的，无法从过于精炼的数据中受益。因此，为了节省数据和资源，通常会在一段时间内逐步增加全局批量大小。

同样重要的是，不要以太小的GBS开始，否则进度将不会高效。当数据量过小时，计算（TFLOPS）效率低下，会拖慢一切。这在使用管道并行性（PP）时尤为明显，因为关于PP调优最重要的一点是GPU空闲气泡要小，而GBS越小，这个气泡就越大。

例如，在我们使用PP的情况下，对于BLOOM-176B，经过吞吐量基准测试后，我们发现以GBS=16开始非常缓慢（8 TFLOPs），所以我们最终从GBS=192（73 TFLOPs）开始，然后逐渐增加到GBS=2048（150 TFLOPs）——每9,765,625个样本增加GBS 16。

### 标准初始化（STD Init）

这个超参数非常重要，需要通过数学计算来正确设置。详细信息请参见[标准初始化](instabilities#std-init)。