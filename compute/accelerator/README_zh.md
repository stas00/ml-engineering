# README - 中文翻译

加速器

计算加速器是机器学习训练的主力军。起初只有GPU，但现在还有TPU、IPU、FPGA、HPU、QPU和RDU等，并且还在不断发明新的加速器。

主要的机器学习工作负载包括训练和推理。还有一种微调工作负载，通常与训练相同，除非进行非常轻量级的[LoRA风格](https://arxiv.org/abs/2106.09685)微调。这种微调所需的资源和时间要少得多。

在语言模型中，推理过程中生成是一次一个token进行的。因此，需要重复数千次相同的`forward`调用，每次调用涉及相对较小的矩阵乘法（矩阵乘法或GEMM）。这可以在加速器上完成，如GPU，或者在一些最新的能够高效处理推理的CPU上完成。

在训练过程中，整个序列长度在一个巨大的`matmul`操作中处理。如果序列长度为4k，则训练同一模型的计算单元需要处理比推理多4k倍的操作，并且要快速完成。加速器在这方面表现出色。事实上，矩阵越大，计算效率越高。

另一个计算差异在于，虽然训练和推理在`forward`过程中执行相同数量的`matmul`，但在仅用于训练的`backward`过程中，为了计算输入和权重的梯度，会额外进行两次`matmul`。如果使用激活函数重新计算，还会额外执行一次`forward`。因此，训练过程需要比推理多3-4倍的`matmul`。

## 子部分

通用：
- [基准测试](benchmarks)

NVIDIA：
- [解决NVIDIA GPU问题](nvidia/debug.md)

AMD：
- [解决AMD GPU问题](amd/debug.md)
- [AMD GPU性能](amd/performance.md)

## 高端加速器现状概览

尽管未来可能会有所变化，但与消费级GPU市场不同的是，截至撰写本文时，高端加速器并不多。如果你租用了云服务，大多数供应商提供的少数几种加速器大致相同。

GPU：
- 目前，机器学习云和高性能计算集群已经开始从NVIDIA A100过渡到H100，由于NVIDIA GPU的短缺，这一过程可能需要几个月的时间。H200预计将在2024年第四季度推出。B100、B200、GB200在2024年第一季度宣布，但由于生产延迟，我们可能要到2025年中期才能使用这些产品。
- AMD的MI300X现在已在二级云提供商中广泛可用。MI325X预计也将很快上市。

HPU：
- 英特尔的Gaudi2正在英特尔云中逐渐出现——有大量型号。也可以通过Supermicro、WiWynn和其他即将推出的公司实现本地部署。
- Gaudi3预计将在2024年某个时候上市。

IPU：
- Graphcore提供了其IPU产品。您可以通过他们的云笔记本在[Paperspace](https://www.paperspace.com/graphcore)试用这些产品。

TPU：
- 谷歌的TPU当然可用，但由于只能租赁，软件在GPU和TPU之间转换并不容易，许多（大多数？）开发者仍然停留在GPU领域，因为他们不想被锁定在一个由谷歌垄断的硬件上。

关于Pods和机架：
- Cerebras的WaferScale Engine (WSE)
- SambaNova的DataScale
- 由上述GPU组成的多种不同的Pod和机架配置，具有超快的互连。

截至2024年第三季度，这就是全部内容。

由于大多数人租用计算资源，而且从未见过实际硬件的样子，以下是8xH100节点的物理外观（这是戴尔PowerEdge XE9680机架服务器的GPU托盘）：

![nvidia-a100-spec](images/8x-H100-node-Dell-PowerEdge-XE9680.png)

## 术语表

- CPU：中央处理器
- FPGA：现场可编程门阵列
- GCD：图形计算芯片
- GPU：图形处理单元
- HBM：高带宽内存
- HPC：高性能计算
- HPU：Habana Gaudi AI处理器单元
- IPU：智能处理单元
- MAMF：最大可实现矩阵乘法浮点运算次数
- MME：矩阵乘法引擎
- QPU：量子处理单元
- RDU：可重构数据流单元
- TBP：总板载功率
- TDP：热设计功率或热设计参数
- TGP：总图形功率
- TPU：张量处理单元

## 最重要的一点理解

我将在本书中多次强调——仅仅购买或租用最昂贵的加速器并不能保证高投资回报率（ROI）。

对于机器学习训练来说，高ROI的两个指标是：
1. 训练完成的速度，因为如果训练时间比计划长2-3倍，你的模型在发布之前可能已经过时了——在这个超级竞争激烈的机器学习市场中，时间就是一切。
2. 训练模型所花费的总金额，因为如果训练时间比计划长2-3倍，你最终会花费2-3倍的资金。

除非购买或租用的其他硬件经过精心选择以匹配所需的工作负载，否则加速器很可能会闲置很多，时间和资金都会损失。最关键的组件是[网络](../../network)，其次是[存储](../../storage)，而[CPU](../cpu)和[CPU内存](../cpu-memory)是最不关键的（至少对于典型的训练工作负载而言，任何CPU限制都可以通过多个`DataLoader`工作者来补偿）。

如果租用计算资源，通常没有选择的自由——硬件要么是固定的，要么某些组件可以更换，但选择有限。因此，有时选定的云供应商提供的硬件不够匹配，这时最好寻找其他供应商。

如果你购买自己的服务器，我建议在购买前进行深入的尽职调查。

除了硬件，你当然还需要能高效部署硬件的软件。

本书将在各个章节中讨论硬件和软件方面的问题。你可以从[这里](../../training/performance)和[这里](../../training/model-parallelism)开始阅读。