# ai-battlefield - 中文翻译

## AI战场工程——你需要知道的一切

本章是某人对ML/AI工程现实情况的主观概述，这可能是另一个人的现实情况。目的是帮助你开始提出正确的问题并满足你的ML工程需求。

## 基础知识

### 在AI竞赛中什么最重要？

训练：

1. 能多快训练出一个更好的模型（先发优势）
2. 花了多少钱（训练后是否还有钱支付人才的工资）

推理：

1. 快速延迟（用户习惯于毫秒级响应时间，如果响应时间达到秒级他们会离开）
2. 高吞吐量（可以同时处理多少查询）
3. 每个用户花费多少钱（我们能否租用更多GPU来获取更多用户和/或改善（1）和（2））

### 大型语言模型训练的需求是什么？

1. 快速计算，主要由矩阵乘法主导
2. 快速足够的内存、IO和网络以及CPU来支持计算

推论：如果你购买或租赁硬件时投资最快的加速器，但在其他组件上节省成本，那么你可能会浪费钱，并且可能无法赢得比赛，因为训练时间会更长。

### ML的主力军是什么？

- 加速器或处理单元完成了大部分工作。
- 由于ML进行大量并行处理（[SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)），最初使用了GPU，但现在除了TPU、IPU、FPGA、HPU、QPU、RDU等，最近的CPU也开始被用作加速器，特别是在推理方面。

[更多信息](../compute/accelerator)。

### 驱动AI的实体

- AI公司 - 训练模型/构建产品围绕自训练或他人训练的模型，在内部进行研究。
- 学术界 - 进行大量研究并撰写论文。很多新想法都是在这里产生的。
- AI爱好者 - 可获得大量善意资源，有些人将资源和人才聚集起来训练开放访问模型，通过HPC和偶尔的云或大学集群捐赠计算能力。
- 创业者 - 有很多低垂果实可摘 - 创意服务再销售，制作基于ML的应用程序，并利用各种巧妙组合现有资源创造惊人成果。

### 信息共享

- 几乎所有参与AI领域的人都在与社区分享大量发现，这一点非常令人惊讶。
- 当然，公司不会披露所有知识产权，但很多以知识或模型权重的形式分享。
- 发布大量知识产权和模型的公司往往会吸引更高素质的人才。
- Twitter似乎是必须关注的中心平台，以便了解最新动态。

### AI泡沫

- [互联网泡沫](https://en.wikipedia.org/wiki/Dot-com_bubble)发生在1995-2000年期间。目前AI领域也出现了类似的情况。

- 很多资金可用于创建新的初创公司或提升现有的公司。筹集数百万美元相对容易。

- 由于我们正处于AI行业的狂野西部阶段，很难预测未来，因此只要听起来合理，任何创业想法都可以尝试。

- 区分AI泡沫和互联网泡沫的是，运营互联网公司实际上不需要太多资金——大多数筹集的资金用于营销和一些员工开支，几乎没有用于计算。AI公司需要数百万美元，因为训练大型语言模型需要大量的计算资源，而且这些计算资源非常昂贵。例如，1块NVIDIA H100的成本约为3万美元，公司可能需要512块这样的设备，总成本为1500万美元（不包括其他硬件组件及相关费用）！

## ML工程师的天堂和地狱

这是我个人基于LLM/VLM训练的天堂和地狱。你的体验可能会有所不同。

### ML工程师的天堂

1. 一个设计良好的HPC，或者完全托管的基于云的集群，有人能及时维护硬件和系统。

   我只需要带上我的训练软件就可以开始训练，这本身已经是一项复杂的工作，需要特殊技能。

2. 有大量的节点可供专属无限使用

3. 快速的节点间连接，不会限制加速器的速度，并且不会与其他用户共享

4. 大型的本地超级快速NVME共享文件系统，可以容纳数据集和检查点

5. 最简化的Linux系统，带有SLURM和少量软件，以便启动训练任务

6. `sudo`权限，便于团队协作

### ML工程师的地狱

1. 云或内部集群，你必须做一切事情——系统管理、更换硬件、处理中断等。此外还要进行训练。

2. 小型缓慢的共享文件系统（NFS？），需要从云端拉取数据和保存检查点

3. 慢速的节点间连接导致加速器利用率低下

4. 节点间与其他用户共享导致网络不稳定和不可预测

5. 复杂的云控制台，需要无数屏幕和步骤才能设置简单的事情

6. 不能快速更换故障硬件

7. 需要与其他用户共享节点——训练任务之间有等待时间

8. 其他并发用户可能耗尽整个磁盘空间，导致训练崩溃

9. 无法终止团队中其他人开始并睡着的任务


## 获取计算资源

有三种主要选择：

- 租用云服务
- 分享HPC
- 购买硬件


### 租用云服务

这是当前获取计算资源的主要方式。

优点：

- 容易扩展或收缩集群规模
- 容易在几年内升级到新一代硬件
- 集群管理可以轻松外包

缺点：

- 如果不谈判长期（1-3年）合同，租用几百个加速器会很贵
- 你会被诱惑购买很多工具和服务，而这些工具和服务你可能并不需要
- 无论你是否充分利用集群，都会被收费


### 使用HPC

HPC数量不多，所以可用资源有限。

优点：
- 为你管理一切——你只需带上自己的软件进行训练和一点点[SURAM](../orchestration/slurm)知识来启动任务
- 经常由当地政府/大学赞助——可能以较少的钱甚至免费完成任务（例如，我们在[JeanZay HPC](http://www.idris.fr/eng/jean-zay/)上免费训练了[BLOOM-176B](https://huggingface.co/bigscience/bloom)！）

缺点：
- 需要与其他团队共享计算资源——这意味着短任务时间和可能的长时间等待——可能难以快速完成训练
- 节点间的网络可能不稳定，因为会被其他团队使用
- 必须遵守HPC的规则（例如，没有sudo权限和各种其他规则）
- 从某种意义上来说，HPC集群就是它本身的样子——你不能让网络更快，而且通常安装一些软件都可能很棘手。


### 购买硬件

主要是大学购买并构建自己的集群，一些大公司也会这样做。

优点：

- 如果你能连续使用超过几年，总成本会比租用便宜
- 容易提供快速的本地存储——好的NVME RAID比在线存储更便宜更快

缺点：

- 几年后你就会被锁定在过时的硬件上——可能能够转售
- 必须购买超过所需的硬件——硬件往往容易损坏，特别是24/7运行时，返修可能需要几周
- 必须雇佣人才来管理内部解决方案
- 必须考虑冷却、电费、保险等问题


### 管理计算资源

- 除非使用完全托管的HPC计算资源，否则你绝对需要聘请系统管理员。可能会觉得你的ML工程师可以在训练任务之间兼顾系统管理，但他们会在管理磁盘空间、处理问题节点、要求用户行为等方面浪费大量时间。


## 技术需求

### 能否快速喂养这个熔炉？

想象一下蒸汽机车——引擎很棒，但如果司炉不能快速地将煤铲入，火车就无法快速行驶。

![](images/640px-Baureihe52Heizer.jpg)

[来源](https://commons.wikimedia.org/wiki/File:Baureihe52Heizer.jpg)

这就是目前ML硬件的状态：瓶颈在于移动数据而不是计算。

- 加速器每两年约提高2倍速度（[摩尔定律](https://en.wikipedia.org/wiki/Moore%27s_law)）
- 网络和内存却不是！现在两者已经成为计算瓶颈
- 如果数据加载器必须从云端拉取数据，IO也可能成为另一个瓶颈
- CPU只要有足够的cpu-核心用于数据加载器工作进程和主进程就没问题

推论：研究整个机器，而不仅仅是它的引擎。

一个疯狂的想法：如果实际数据移动速度能跟上计算速度，老一代的GPU可能也能工作得很好。并且如果能在相同成本下获得三倍的GPU，你可能可以更快完成训练并且成本更低。

## TFLOPS

- 一旦选择了架构和模型大小以及希望训练的令牌数量，你立即就知道实现这一目标所需多少计算。具体来说，你现在可以计算出需要多少浮点运算（[详见](../training/performance/README.md#tflops-as-a-performance-metric)）。

- 所缺少的部分是对比不同计算提供商的硬件每秒能计算多少浮点运算（TFLOPS）及其单位成本，现在你可以估算出训练的大致成本。

  1. 根据所考虑解决方案的TFLOPS计算所需训练时间：
     `总TFLOPS需求 / 此计算单元的TFLOPS = 时间（秒）`
     例如，结果为604800秒或7天。

  2. 查看使用此计算解决方案7天的成本，现在你知道训练此模型的总成本。

  3. 查看其他方案并进行相同的计算——选择最佳选项。

- 如前所述，时间非常重要，所以即使某个解决方案更昂贵，你仍然可能选择它，因为尽快完成训练很重要，因为你希望尽早进入市场。

不幸的是，这种数学计算只部分正确，因为公布的峰值TFLOPS通常是无法实现的。详细内容见MFU部分。


### 模型FLOPS利用率（MFU）

正如前一部分提到的，一些（大多数？）供应商发布的是不切实际的峰值性能TFLOPS——它们无法实现。

模型FLOPS利用率（MFU）是衡量加速器利用率的指标。以下是计算方法：

1. 通过计算单次训练迭代所需的浮点运算数，然后除以该迭代所需的时间来测量实际TFLOPS。
2. 将实际TFLOPS除以公布的TFLOPS得到MFU

示例：假设你在BFLOAT16精度下进行训练：

- 如果单次迭代需要624万亿浮点运算，并且运行时间为4秒，则我们知道我们得到了：`624/4=156` 实际TFLOPS
- 现在BF16@A100被[公布为312TFLOPS](https://www.nvidia.com/en-us/data-center/a100/)，所以`156/312=0.5` 给我们50%的MFU。

实践：

- 使用NVIDIA GPU，如果你在一个多节点设置中拥有一个大型模型并且MFU高于50%，你已经在做得很出色了
- 最近的效率更高的可扩展性解决方案的进步不断提高MFU
- 慢速网络和低效框架或未调优配置降低MFU

因此，一旦你知道了MFU，你现在就可以调整前一部分中的成本估算。在示例中，我们说它需要7天来训练，但如果MFU是50%，则意味着需要14天来训练。


### 移动数据

为什么无法达到公布的TFLOPS？这是因为数据在加速器内存和计算之间移动需要时间，而且将数据从磁盘和其他GPU移动到加速器内存需要更多时间。

- 对于加速器内存带宽，几乎无能为力——只能编写更高效的软件使数据更快地移动到/从加速器——提示：融合和自定义编写的内核（如[torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html)和[flash attention](https://github.com/Dao-AILab/flash-attention)）

- 如果你只有一个GPU且模型适合其内存，你不必担心网络——加速器内存是唯一的瓶颈。但如果你需要[将模型跨多个GPU分片](../training/model-parallelism)，网络将成为瓶颈。

- 节点内网络——非常快，但对于大型模型很难充分利用——[张量并行化](../training/model-parallelism#tensor-parallelism)和[序列并行化](../training/model-parallelism#sequence-parallelism)解决了部分问题。([更多信息](../network/README.md#intra-node-networking))。

- 跨节点网络——在大多数服务器设置中通常太慢——因此这是需要研究的关键组件！高效框架成功地通过重叠计算和通信来部分隐藏通信开销。但如果通信时间长于计算时间，通信仍然是瓶颈。[更多信息](#inter-node-network)。

- 存储IO对于喂食DataLoader工作者和保存检查点至关重要。[更多信息](#storage)。

  1. 通常有足够的DL工作者，DataLoader增加的开销很小。
  2. 当检查点正在保存时，除非使用某种异步保存解决方案，否则加速器会空闲，因此快速IO在这里至关重要。


## 关键硬件组件

### 加速器

目前最常用的可用于训练、微调和推理ML模型的加速器如下：

广泛可用：

  * NVIDIA H100正在逐渐取代A100。我们希望H200能尽快取代H100（Q4-2024），因为后者具有更高效的HBM，从而使其更具成本效益。

可用，但会锁定供应商：

  * Google TPUs——速度快！但代价是被锁定到单一供应商和云服务

即将普及：

  * NVIDIA H200——比H100具有更快的HBM和更多的内存——Q4-2024在某些云服务上（并非所有大型云服务商都计划采购）。

  * NVIDIA B100、B200和GB200——预计在2025年初上市。

  * AMD MI300X ≈ H100——Tier 2云服务商从Q2-2024开始提供这些设备——你需要使用最新的ROCm并激活许多优化才能获得高TFLOPs。

  * Intel Gaudi3 > H100——在Intel的云服务上可用。

  * GraphCore IPU——非常难找，PaperSpace上有。

  * Cerebras WaferScale Engine——在Cerebras的云服务上可用。

有关完整的列表和最近宣布的加速器，请参阅[加速器](../compute/accelerator)。

### 加速器互操作性

一般来说，大多数（所有？）加速器都被主要框架（如PyTorch或TensorFlow）支持，并且只要不使用任何特定于加速器的功能，相同的代码应该可以在所有地方运行。

例如，如果你的PyTorch应用程序调用了`torch.mm`——它应该可以在所有地方运行，但如果它包含自定义CUDA内核，它只会适用于NVIDIA GPU，并且可能适用于最新的AMD MI系列。

- NVIDIA GPUs：所有基于[CUDA](https://developer.nvidia.com/cuda-toolkit)，这是大多数训练框架支持的。你可以很容易地在不同的NVIDIA GPU之间切换，大多数事情都能正常工作。

- AMD MI250/MI300X：使用[ROCm](https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/)，你可以像运行CUDA软件一样运行大多数PyTorch应用。这是唯一可以与NVIDIA堆栈互操作的加速器。

- Intel Gaudi2/Gaudi3：如果你使用HF Transformers/Diffusers，可以使用[optimum-habana](https://github.com/huggingface/optimum-habana)。如果你使用HF Trainer与NVIDIA GPU，切换到Gaudi2进行训练/推理应该相对容易。

- GraphCore IPU：也可以通过[poptorch](https://github.com/graphcore/poptorch)在PyTorch中运行。

- Cerebras：也在通过[Cerebras Software Platform (CSoft) via XLA](https://www.cerebras.net/blog/supporting-pytorch-on-the-cerebras-wafer-scale-engine/)提供PyTorch支持。

总的来说，大多数ML代码可以编译成跨平台格式，如[Open Neural Network Exchange (ONNX)](https://en.wikipedia.org/wiki/Open_Neural_Network_Exchange)，可以在多种加速器上运行。这种方法通常更多地用于推理工作负载。