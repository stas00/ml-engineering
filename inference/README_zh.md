# README - 中文翻译

## 推理

XXX：本章正在建设中——有些部分已完成，有些部分刚开始，还有很多部分尚未开始，但已经完成的有用部分足以使其成为一个很好的阅读材料。

## 术语表

- CLA：跨层注意力
- FHE：全同态加密
- GQA：分组查询注意力
- ITL：令牌间延迟
- KV：键值
- LPU：语言处理单元™
- MHA：多头注意力
- MPC：安全多方计算
- MQA：多查询注意力
- PPML：保护隐私机器学习
- QPS：每秒查询数
- TPOT：每个输出令牌所需时间
- TTFT：生成第一个令牌所需时间

参见 [概念](#concepts) 获取更多类似词条的解释。

## 概念

### 预填充和解码

在进行推理时，有两个阶段：

#### 预填充

预填充：由于所有提示令牌已知，一次性处理完整的提示长度（类似于训练）并缓存中间状态（KV 缓存）。此阶段贡献的延迟非常小，因为即使是一个长度为1000的提示也可以快速处理，只要内存足够。

#### 解码

解码：新令牌生成是一次一个地进行（回归方法），基于之前的所有令牌（提示和到目前为止生成的新令牌）。因此，这个阶段对生成的延迟贡献最大，因为它不像预填充那样可以并行化。

### 在线推理与离线推理

当用户实时发送查询时，这就是在线推理，也称为部署或交互式推理。示例包括聊天机器人、搜索引擎和通用REST API。在这种情况下，通常会运行一个推理服务器，并且可能有各种客户端对其进行查询。

当你有一个包含数百或数千个提示文件需要运行推理时，这就是离线推理。示例包括基准评估和合成数据生成。在这种情况下，通常不需要推理服务器，推理直接在发送查询的同一程序中运行（客户端和服务端在一个应用程序中）。

这两个主要用例经常针对不同的性能指标进行优化——在线推理需要非常低的TTFT和低延迟，而离线推理则需要高吞吐量。

### 上下文关联

这是给预训练模型提供其训练期间不可用的额外信息的过程。
例如，[输入关联任务](#输入关联任务) 会给模型提供大量额外的信息。非零样本提示会根据示例调整默认模型行为。提示工程都是关于在推理过程中引导模型以某种方式表现。

检索增强生成（RAG）是上下文关联模型的主要技术之一，因为它为推理过程提供了与提示相关的附加数据。目的是让模型更重视这些信息而不是它经过大规模压缩训练的数据。

微调到不同知识领域是另一种上下文关联方法，我们更新模型使其在新的数据集上上下文化，这个数据集可能与基础模型最初训练的数据域完全不同。

上下文关联可以理解为提供背景。任何人都可以证明，理解问题的背景更容易回答问题。这同样适用于模型生成。背景越好，生成的内容就越相关。

在多模态使用场景中，文本提示附带的图像或视频可以作为上下文。

### 任务

#### 输入关联任务

输入关联任务是指生成的回答主要来源于提示，即主要的知识来源包含在提示中。这些包括：

- 翻译
- 摘要
- 文档问答
- 多轮对话
- 代码编辑
- 语音识别（音频转录）

### 批处理

一次处理解码阶段一个令牌是非常加速器效率低下的。批量处理多个查询可以提高加速器利用率并允许同时处理多个请求。

最大可能的批量大小取决于加载模型权重后还剩下多少内存以及填充KV缓存所需的中间状态。

#### 静态批处理

这是一种简单的批处理方式，其中前N个查询被一起处理——这里的问题是如果许多查询已经完成了生成，它们将不得不等待最长的生成查询完成后再返回给调用者——大大增加了延迟。

#### 连续批处理或飞行中批处理

连续批处理或飞行中批处理是一种过程，在该过程中，生成引擎一旦完成结果就立即移除它们，并用新查询替换它们，而不必等待整个批次完成。因此，在批次中的位置0的序列可能正在生成它的第10个令牌，而位置1的序列可能刚刚开始生成第一个令牌，位置3正在生成最后一个令牌。

这种方法提高了响应时间，因为已经完成的序列无需立即返回，而且新的提示也不必等待下一个批次可用。当然，如果所有的计算都完全繁忙，并且没有新的空间在批次中，那么一些请求将不得不等待计算开始处理那些请求。

### 分页注意力

分页注意力在推理服务器中非常流行，因为它通过像操作系统使用分页一样接近加速器内存，从而实现了非常高效的加速器内存利用，允许动态内存分配并防止内存碎片。

### 解码方法

主要的解码方法有：[贪婪解码](#贪婪解码)，[束搜索](#束搜索) 和 [采样](#采样)。

#### 贪婪解码

贪婪解码是指模型总是选择概率最高的令牌。这是最快的解码方法，但它并不总能生成最佳结果，因为它可能会选择一条不太理想的令牌路径并错过未来一系列令牌的好结果。

这种方法的主要问题之一是创建循环，即句子不断重复。

#### 束搜索

束搜索通过同时生成多个输出来克服贪婪解码的局限性，所以不是遵循最高概率，而是使用束大小为3时，每次新令牌跟随前三名的概率，然后从9个子路径中保留前3条子路径（3*3），这些子路径导致链中所有令牌的总概率最高。最后选择总概率最高的路径。

这个方法比贪婪解码慢，因为它必须生成n倍的令牌并且需要n倍的内存。

#### 采样

基于采样的解码引入了随机性。

但是，当然，随机选择单词不会产生好的结果，所以我们仍然想要像贪婪解码那样的确定性，但通过添加受控的随机性使其更加有趣/生动。

最常见的采样方法是：

- **Top-K 采样** 方法基于其对数概率选择前k个令牌，然后从这些令牌中随机挑选一个。
- **Top-p 采样**（也称为**核采样**）类似于Top-K 采样，但K对于每个后续令牌会变化，它是通过将顶部令牌概率累加直到达到阈值`p`来计算的。因此，如果模型对某些预测非常确定，只有这些预测才会被考虑。

#### 温度

温度是[Top-p](#采样)采样策略的一个组成部分，其影响如下，具体取决于其值：

- `t==0.0:` 最终会选择具有最高概率的令牌——没有随机性——与贪婪解码相同——精确应用场景。
- `t==1.0`: 对采样没有影响——保留原始训练分布——平衡相关性和多样性应用场景。
- `0.0<t<1.0`: 对数概率被进一步拉开，因此越接近0.0，随机性越少——介于精确和平衡应用场景之间。
- `t>1.0`: 对数概率被拉近，创造更多的随机性——创造性应用场景。

为了真正理解影响，温度因子通常在Softmax操作之前或作为一部分应用到对数概率上。

``` 
logits = math.log(probs) / temperature
```
由此可见`t=1.0`没有任何影响，`t=0`会使最高对数概率渐近趋向无穷大（避免了除以零），而`t<1.0`和`t>1.0`会相应地使值拉开或拉近——因为`log`。

温度对贪婪解码、束搜索和Top-K采样策略没有影响，因为它影响对数概率之间的距离，而所有这些策略都基于顺序使用最高概率，温度不会改变概率顺序。而Top-p采样允许根据总概率允许或不允许更多竞争者进入随机抽样的子集——因此概率越接近（高温），可能性越大。

除了`t==0.0`和`t==0`之外，没有硬性规定值可以复制，你需要为每个应用场景进行实验，找到最适合你的需求的值——尽管如果你在网上搜索，肯定会找到一些不同应用场景的良好基线。

