# README - 中文翻译

## 节点间和节点内网络硬件

### 网络调试
### 网络基准测试

## 引言

仅仅购买或租用昂贵的加速器以快速训练和推理模型是不够的。您还需要确保您的存储I/O、CPU和网络足够快，以“喂饱加速器”。如果这一点没有得到保证，那么昂贵的加速器将会被闲置，导致金钱损失、训练时间延长以及推理吞吐量降低。虽然这可能是任何其他组件的问题，但在训练过程中，网络通常是瓶颈（假设您的DataLoader足够快）。

如果您可以将模型装入单个加速器中，则无需过多担心。然而，如今大多数模型需要多个加速器来加载，并且LLM/VLM模型在训练时需要多个计算节点，有些甚至在推理时也需要。

大多数计算节点包含8个加速器，一些包含4个，还有一些包含16个，甚至更多。最近有一些节点只有一个超级加速器。

当模型跨越多个加速器并且没有离开单个节点时，您只需要关注快速的[节点内网络](#节点内网络)。一旦模型需要多个节点，这在训练时很常见，因为可以通过多个副本并行化以加速训练，那么快速的[节点间网络](#节点间网络)就变得至关重要。

本文涵盖了这两种类型的网络硬件，报告其理论和实际带宽，并解释它们之间的相互作用。

## 术语和概念

您可以暂时忽略这里列出的许多概念和缩写，直到需要时再返回这里查看。

- ALU：算术逻辑单元
- AR：自适应路由（但也可能指聚合路由器）
- DMA：直接内存访问
- EFA：弹性结构适配器
- HCA：主机通道适配器
- IB：InfiniBand
- MFU：模型浮点运算利用率（例如，在A100上半精度的MFU为0.5是因为达到了156TFLOPs，而峰值半精度规格为312TFLOPs，因此 `156/312=0.5`）
- NIC：网络接口卡
- OPA：Omni-Path架构
- OPX：Omni-Path Express
- OSFP：八通道小型可插拔（收发器）
- RDMA：远程直接内存访问
- RoCE：融合以太网上的RDMA
- RoE：以太网上的RDMA
- SHARP：可扩展分层聚合协议
- VPI：虚拟协议互连
- xGMI：插座到插座全局内存接口

速度相关：
- 单向：从一个点到另一个点在一个方向上传输（A -> B）
- 双向，全双工：从一个点到另一个点在两个方向上传输（A <-> B），通常比单向传输速度快两倍
- GBps, GB/s：每秒传输的吉字节数（1GBps = 8Gbps）在信道中
- GT/s：每秒传输次数 - 每秒发生的传输数据操作次数。
- Gbps, Gb/s：每秒传输的吉比特数（1Gbps = 1/8GBps）在信道中
- 割集带宽：将网络分为两部分所需的最小链路数（不一定相等）。这些链路的带宽称为割集带宽 - 这通常用于衡量实际网络带宽）。有时也被称为最坏情况下的网络容量。这里有一个[好的答案](https://networkengineering.stackexchange.com/a/29662/93656)，它解释了这一概念及相关概念，但除非您需要理解，否则不太可能需要知道这些细节，因为您的集群拓扑很可能已经由提供商完成。
- 自适应路由改进静态路由，以实现无序的数据包在网络中的传输。数据包在每个交换机处进行负载均衡，以更好地分配网络工作负载。
- [远程直接内存访问](#rdma网络)

脚注：在以下部分，请注意1GBps = 8Gbps。

### 单向与双向（全双工）

大多数基准测试/带宽测量工具会报告单向带宽。因此，在查看单向与双向（全双工）速度时要小心。通常后者大约是前者的两倍。

如果您在设置上测量的带宽约为标称速度的40％，请仔细检查是否标称速度表示全双工，如果是，则将其减半，然后您的测量带宽应该约为80％，这是预期的。

案例研究：有一段时间我无法理解为什么我在一台具有600GBps节点内速度的A100节点上运行nccl-tests的所有归约基准测试时只能获得235GBps（40％），直到Horace He友好地指出我应该查看单向速度，即300GBps，然后我得到了理论规范的80％，这符合预期。

## 集群网络

集群中的每个节点有3个网络，每个网络的速度都与其他网络相差很大。

1. [前端网络](#前端网络)
2. [后端网络](#后端网络)
3. [带外网络](#带外网络)

### 前端网络

前端网络通常用于互联网连接（例如下载Python包和云存储）、分布式网络存储（例如检查点和数据集）和编排（例如SLURM和Kubernetes）。目前，一个典型的节点可能会有一个100-400Gbps的连接。

脚注：并非所有集群都有外部互联网连接可用，例如许多高性能计算环境仅通过特殊的CPU-only节点提供外部访问。

### 后端网络

后端网络用于GPU之间的连接，允许训练和推理扩展到多个加速器（例如所有归约、所有聚集和其他集体通信）。这是AI集群最重要的一部分。通常这将是[InfiniBand](#infiniband)或[RoCEv2以太网](#rdma网络)。然后它分解为[节点内网络](#节点内网络)和[节点间网络](#节点间网络) - 同一节点内的GPU之间通信速度通常比跨节点的GPU更快。这里的典型最高速度目前为节点内约5600Gbps，节点间约3200Gbps。每个加速器至少有一个后端连接，有时每个加速器会有多个连接，特别是如果使用低带宽NIC时。

脚注：并非所有提供商都会匹配行业标准的网络速度 - 在某些情况下，节点间网络速度可能慢至10倍。因此，请始终检查您所获得的速度。

### 带外网络

带外（OOB）网络用于启动后端网络、监控节点健康状况、远程重新映像节点等。它通常使用一个较慢的1Gbps以太网连接。

## RDMA网络

远程直接内存访问就像节点上的DMA（直接内存访问），但跨节点进行。它允许在不使用本地处理器、操作系统内核和缓存的情况下在节点之间交换数据，这正是TCP/IP所使用的。主要有三种实现方式：

1. InfiniBand
2. 融合以太网上的RDMA（RoCE）（基于IB或UDP的RDMA）
3. iWARP（基于TCP的RDMA）

这里有一篇[很好的概述文章](https://community.fs.com/article/roce-vs-infiniband-vs-tcp-ip.html)。

## 节点内网络

有许多平台/解决方案提供了节点内网络：

1. 通用：[PCIe](#pcie)
2. NVIDIA：[NVLink](#nvlink) 和 [NVSwitch](#nvswitch)
3. AMD：[Infinity Fabric](#infinity-fabric--xgmi)
4. Intel：[Gaudi2](#gaudi2)，[Gaudi3](#gaudi3)

### 全对全带宽

以下是当前解决方案的节点内单向理论全对全峰值带宽比较表，按带宽排序：

| 互连           | 加速器     | GBps |
| :------------- | :--------- | ----: |
| NVIDIA NVLink 5 | B200, B*   | 900.0 |
| Intel           | Gaudi3     | 600.0 |
| NVIDIA NVLink 4 | H100, H*   | 450.0 |
| AMD XGMI        | MI325X     | 448.0 |
| AMD XGMI        | MI300X     | 448.0 |
| AMD XGMI        | MI250X     | 350.0 |
| NVIDIA NVLink 3 | A100       | 300.0 |
| Intel           | Gaudi2     | 300.0 |
| PCIe 5          |            |  63.0 |
| PCIe 4          |            |  31.0 |

注释：

- NVSwitch 的速度与该代的NVLink相同。有关NVSwitch的信息，请参阅[NVSwitch](#nvswitch)和关于节点间[NVLink Switch](#nvlink-switch)的信息。
- 请注意，当规范说明单向与双向（全双工）速度时，如果在线规范没有明确声明方向性，请查找答案。在下面的一些表格中，我不得不研究许多文档来弄清楚这一点，因为一些供应商在发布的规范中省略了这一重要信息。我甚至不得不编辑几个维基页面以添加缺失的信息。记住，对于供应商来说，越大越好，所以几乎总是他们会使用全双工数字，这通常比单向数字大两倍。

### 对等带宽

一些供应商的全对全和对等带宽（GPU对GPU）相同，而另一些则不同。例如，AMD MI3\*的GPU对GPU带宽为64GBps，但在8个加速器的板上总共有448GBps，因为 `64*7=448`。

以下是当前解决方案的节点内单向理论对等带宽比较表，按带宽排序：

| 互连           | 加速器     | GBps |
| :------------- | :--------- | ----: |
| NVIDIA NVLink 5 | B200, B*   | 900.0 |
| Intel           | Gaudi3     | 600.0 |
| NVIDIA NVLink 4 | H100, H*   | 450.0 |
| NVIDIA NVLink 3 | A100       | 300.0 |
| Intel           | Gaudi2     | 300.0 |
| AMD XGMI        | MI325X     |  64.0 |
| AMD XGMI        | MI300X     |  64.0 |
| AMD XGMI        | MI250X     |  50.0 |

当对等带宽远低于全对全带宽时，这意味着如果您不使用节点上的所有加速器，则应用的带宽将大大降低，如果加速器之间必须通信，则应用程序性能将受到影响。

为了验证这一点，[all_reduce_bench.py](benchmarks/all_reduce_bench.py)在具有4GB有效载荷的8个GPU AMD MI300X节点上运行，`busbw` 测量结果如下：

- 2个GPU： 47.671 GBps
- 8个GPU： 312.912 GBps

即2个GPU比8个GPU慢6.5倍。

因此，如果您需要部署TP=2、TP=4或ZeRO-DP/FSDP在2个或4个GPU上，无论是训练还是推理，网络将成为瓶颈。如果您使用TP=1或TP=8或ZeRO-DP/FSDP在8个GPU上，或者在1个GPU副本上使用DP，则没有问题。（如果您不确定TP/ZeRO-DP/DP是什么意思，请参见[model-parallelism](../training/model-parallelism)。）

您将在以下章节中找到每种技术的详细分析。