# README - 中文翻译

## 存储：文件系统和输入输出

## 3 机器学习输入输出需求

在机器学习负载中有三种不同的输入输出需求：

1. 你需要能够快速地为数据加载器提供数据——（超级快的读取，不关心写入速度）——需要数小时到数天的持续负载。
2. 你需要能够快速地写入检查点——（超级快的写入，稍微快一点的读取，因为你可能要恢复几次）——需要突发写入——你希望写入速度极快以避免阻塞训练时间（除非你使用某种CPU卸载来迅速解除训练阻塞）。
3. 你需要能够加载和维护代码库——（中等速度的读写）——这也需要共享，因为所有节点都需要看到相同的代码库——由于只会在开始或恢复时发生，所以这种情况不频繁。

正如您所见，这三种需求在速度和可持续负载方面都有非常不同的要求，因此理想情况下，您应该有三种不同的文件系统，每种都针对所需的用例进行优化。

如果您有无限的资金，当然，可以购买一个单一的超快读取、超快写入的文件系统，可以连续几天不间断运行。但对于我们大多数人来说，这是不可能的，因此获取两种或三种不同类型的分区，这样最终支付的费用会少得多，是一个更明智的选择。

## 术语表

- NAS：网络附加存储
- SAN：存储区域网络
- DAS：直接附加存储
- NSD：网络共享磁盘
- OSS：对象存储服务器
- MDS：元数据服务器
- MGS：管理服务器

## 选择哪个文件系统

**分布式并行文件系统是最快的解决方案**

分布式并行文件系统可以在数百到数千个客户端同时访问共享存储时显著提高性能。它们还大大减少了热点问题（某些数据被访问的频率远高于其他数据）。  

我有经验的两个出色的并行文件系统是：

- [Lustre 文件系统](https://www.lustre.org/)（开源）([维基](https://wiki.lustre.org/Main_Page))
- [GPFS](https://en.wikipedia.org/wiki/GPFS)（IBM），最近更名为 IBM Storage Scale，之前称为 IBM Spectrum Scale。

这两个解决方案都存在了20多年以上。两者都是POSIX兼容的。这些并不是简单的创建——您必须设置一个专门用于这些文件系统的整个集群，包括多个仅CPU的虚拟机。只有这样您才能挂载这些文件系统。与较弱的云提供的“内置”解决方案相比，后者只需回答几个屏幕的问题即可激活。创建存储集群时，选择哪些虚拟机执行哪种功能有一整套科学。例如，这里有一个[在GCP上的Lustre指南](https://cloud.google.com/architecture/lustre-architecture)。

案例研究：在JeanZay高性能计算（法国），我们使用384个进程在40秒内并行保存了2.3TB的检查点！这非常快——这是使用NVME驱动器的GPFS。

NASA的集群在使用Lustre时有许多需要注意的问题。[NASA支持](https://www.nas.nasa.gov/hecc/support/kb/lustre-best-practices_226.html)。

GPFS的一些非常有用的优点：
- 如果您有很多小文件，很容易耗尽inode（使用`df -i`检查）。GPFS 5.x永远不会耗尽inode，它会根据需要动态创建更多。
- GPFS不会遇到Lustre那样的问题，即如果其中一个子磁盘满了且没有及时重新平衡，则在达到80％时会耗尽磁盘空间。您可以可靠地使用分配的所有100％存储。
- GPFS不使用中央元数据服务器（或一组这样的服务器），这在处理小文件时经常成为瓶颈。就像数据一样，元数据由存储集群中的每个节点处理。
- GPFS自带了一个原生的NSD客户端，比通用的NFS客户端更好，但也可以使用NFS。

其他我还没有直接经验的并行文件系统：

- [BeeGFS](https://www.beegfs.io/)
- [WekaIO](https://www.weka.io/)
- [DAOS](https://docs.daos.io/)（分布式异步对象存储）（英特尔）
- [NetApp](https://www.netapp.com)

大多数云提供商至少提供这些之一的实现，但并非全部。如果您的云提供商不提供这些之一，并且他们没有足够快的替代方案以满足您的需求，那么您应该重新考虑。

## OK’ish 解决方案

各种云提供商提供了许多OK’ish的解决方案。在承诺任何解决方案之前，请务必认真对其进行基准测试。这些解决方案通常对于处理大文件相当不错，但对于小文件则不太合适。

案例研究：截至本文撰写时，在GCP的Zonal FileStore NFS解决方案上，`python -c "import torch"` 执行需要20秒，这非常慢！一旦文件被缓存，它只需要约2秒。安装包含一些预构建Python包的conda环境可能需要20-30分钟！我们开始使用的这种解决方案对我们的工作非常痛苦且无效。这会影响那些拥有大量Python包和conda环境的人。但是，当然，GCP也提供了更快的解决方案。

## 远程文件系统客户端

您需要选择哪种客户端来连接文件系统到您的虚拟机。

最常见的选择是：[NFS](https://en.wikipedia.org/wiki/Network_File_System)——它已经存在了40年。它引入了额外的开销并减慢了速度。因此，如果有支持您的虚拟机的本地客户端，使用它会获得更高的整体性能。例如，GPFS带有[NSD](https://www.ibm.com/docs/en/linux-on-systems?topic=configurations-network-shared-disk-nsd)客户端，比NFS更好。

## 文件块大小

如果您使用的文件系统使用16MB的块大小，但您的文件平均大小为16KB，那么您将使用的磁盘空间将是实际使用的1000倍。例如，您可能会看到磁盘空间使用量为100TB，而实际使用的磁盘空间仅为100MB。

脚注：在Linux上，本地文件系统通常使用4KB的块大小。

因此，您可能需要满足两种非常不同的需求，并要求两个不同的分区分别针对不同的需求进行优化。

1. 数千到数百万个微小文件——4-8KB的块大小
2. 少数大文件——2-16MB的块大小

案例研究：Python在处理数万个小文件方面非常糟糕，如果您有许多conda环境，您可能会在某些情况下耗尽inode。在JeanZay HPC，我们不得不请求一个特殊的专用分区，用于安装所有conda环境，因为我们不断在普通GPFS分区上耗尽inode。我认为问题在于那些GPFS分区配置了16MB的块大小，因此这不适合4KB大小的文件。

好消息是，现代解决方案已经开始引入动态块大小。例如，最新的GPFS支持子块。因此，例如，可以将GPFS配置为2MB的块大小，并具有8KB的子块，然后将微小的文件打包在一起作为子块，从而不会浪费太多磁盘空间。

## 分布式存储服务器靠近客户端

使用共享分布式存储的集群应将存储服务器放置在靠近使用这些服务器的集群附近。如果运行存储服务器的虚拟机位于多个跳转（交换机）之外，IO延迟可能会很高，交互使用存储可能会令人沮丧地缓慢。例如，当您尝试运行`du`和其他工具访问许多文件的元数据时，您会注意到这个问题。

因此，如果您有控制权，请要求云提供商尽可能接近您的加速器虚拟机在网络距离上分配CPU-only存储服务器虚拟机。

## 云共享存储解决方案

以下是各种云提供商提供的共享文件系统存储解决方案：

- [GCP](https://cloud.google.com/architecture/filers-on-compute-engine)
- [Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-shared)
- [AWS](https://aws.amazon.com/what-is/nas/#seo-faq-pairs#how-can-aws-help-with-storage-solutions)


## 本地存储优于云存储

虽然云存储更便宜，但在训练过程中动态获取和处理训练数据流的想法在面临大量潜在问题的情况下非常有问题。

同样，将检查点动态卸载到云端也是如此。

最好在本地有足够的磁盘空间用于数据加载。

对于检查点，应该有足够的本地磁盘空间以快速可靠的方式保存检查点，然后通过crontab作业或slurm作业将其卸载到云端。始终保留最后几个检查点在本地，以便在作业崩溃时快速恢复，否则从云端获取检查点进行恢复会非常昂贵。

案例研究：我们别无选择，不得不在IDEFICS-80B训练期间使用云存储进行数据加载，因为我们几乎没有本地存储空间，而且由于是多模态数据，我们需要处理多个TB的数据。我们花了数周时间试图使这个解决方案稳健，结果并不理想。最大的问题是当时很难跟踪DataSampler的随机数生成器状态，因为我们使用的解决方案并没有考虑到这一点。因此，很多花费大量时间创建的数据被浪费（未使用），很多数据被重复，所以我们没有一次完整的独特数据。

在某些情况下，人们找到了与基于云的数据集一起工作的良好解决方案，我个人尚未有过顺畅的体验，这就是为什么我主张使用本地存储。如果您找到了一种良好的流式解决方案，可以在不丢失数据和重复数据的情况下正确恢复，并且不需要巨大的本地工作节点，那可能会起作用。

## 注意，您经常只获得支付金额的80%的存储

在计算节点上使用的分布式共享存储存在一个微妙的问题。由于构建大型文件系统的大多数物理磁盘只有0.3-2TB大小，任何这些物理磁盘都可以在联合存储达到满之前先装满。因此，它们需要不断地重新平衡，以确保不会出现一种磁盘装满99%，而其他磁盘只装满50%的情况。由于重新平衡是一种成本较高的操作，类似于大多数编程语言的垃圾回收，它很少发生。因此，如果运行`df`报告90%已满，很可能任何程序在任何给定时刻都会失败。

据与IO工程师交谈得知，接受现实（不知为何客户没有被告知）是只有大约80%的分布式大型存储是可靠的。

这意味着，如果您想要100TB的可靠云存储，实际上需要购买125TB的存储，因为其中的80%将是100TB。因此，您需要计划支付比实际需求多25%的费用。我不明白为什么客户应该为技术缺陷买单，但这确实是现状。

例如，GCP声明只能[89%](https://cloud.google.com/filestore/docs/known-issues#capacity_errors_before_reaching_full_provisioned_capacity)可靠使用存储，尽管在我这里存储在达到83%时已经失败过一次。谷歌公司披露这是一个已知问题，尽管不是在购买存储时披露。因此，建议您购买比实际计划使用的存储多12%的存储，因为他们只能可靠交付89%的存储。

我还与提供管理IBM Storage Scale（GPFS）解决方案的Sycomp工程师交谈过，他们表示GPFS不存在此问题，可以可靠地使用100%的存储。

在某些设置中，如果您通过云提供商API进行备份（而不是直接在文件系统上进行），它们可能会使用相同的分区，当然会消耗磁盘空间，但当您运行`df`时，它不会显示实际的磁盘使用情况——它可能不包括备份的使用情况。因此，如果您的备份消耗了分区的50%。

无论您选择哪种存储解决方案，请向提供商询问可以可靠使用的存储量，以免日后出现意外。

## 注意，某些云提供商的备份使用同一分区

对我来说，这毫无意义，但在某些提供商处，当您使用其工具对分区进行备份时，备份会使用该分区的空间。在某些提供商中，直到您真正使用了30%的分区空间才会意识到这一点。在这些提供商中，运行`df`是毫无意义的，因为它会告诉您可用的磁盘空间，但不会包括备份。因此，您无法知道实际情况。

如果您开始制作备份，突然间所有进程都无法写入，但`df`报告30%的使用率，现在您就会知道为什么会发生这种情况。快照也会使用相同的分区。

假设您为一个100TB的分区付费，您使用了95TB的存储空间，现在想备份它——那么您不能这样做——如果它只剩下5TB的空间，即使压缩数据也无法找到地方放置95TB的数据。

当我发现特定解决方案具有这种不合理的行为时，我会添加如何查看实际磁盘使用情况的指针：
- [GCP FileStore](https://cloud.google.com/filestore/docs/monitoring-instances#free-raw-capacity-percent)（但不适用于Basic Tier）

## 不要忘记校验和

当您从云同步数据时，请确保研究您使用的工具是否检查校验和，否则您可能会传输过程中产生损坏的数据。有些工具会自动检查，而有些则需要启用此功能（因为通常会带来额外的计算成本和传输延迟）。宁可慢，也要安全。

这些通常是MD5和SHA256校验和。如果您的环境安全，通常MD5就足够了，但如果您需要额外的安全性，请使用SHA256校验和。

